\chapter{Zusammenfassung}
\label{zusammenfassung}

\section{Vorteile}
\label{vorteile}
Entscheidungsbäume bieten diverse Vorteile gegenüber anderen Methoden. Einer der offensichtlichsten Vorteile ist die einfache Verständlichkeit und Interpretierbarkeit von Entscheidungsbäumen. \autocite{DataMining} Sie können visualisiert werden und ermöglichen es somit auch Laien mit ihnen zu arbeiten. \autocite{PythonCourseDecisionTrees:online} Außerdem können Entscheidungsbäume aus Datensätzen erstellt werden die nicht normalisiert sind und sowohl numerische als auch kategoriale Attribute enthalten. Dabei können Entscheidungsbäume auch mit sehr großen Datensätzen arbeiten. \autocite{PythonCourseDecisionTrees:online}\\

\section{Nachteile}
\label{nachteile}
Aber natürlich besitzen Entscheidungsbäume auch Einschränkungen. So sind sie relativ inkonsistent, da bereits kleine Änderungen innerhalb des Trainingsdatensatzes zu weitreichenden Veränderungen des Entscheidungsbaums führen können. \autocite{PythonCourseDecisionTrees:online}\\
Besonders bei Entscheidungsbäumen kann es zu Overfitting kommen wodurch reale Daten falsch klassifiziert werden. Allerdings kann diesem Problem mittels Pruning entgegengewirkt werden. \autocite{DataMining}\\
Wenn Datensätze verwendet werden, die kategoriale Attribute mit einer Vielzahl von möglichen Ausprägungen besitzen, ist der Informationsgewinn in Entscheidungsbäumen zugunsten dieser Attribute ausgelegt. \autocite{PythonCourseDecisionTrees:online} Dadruch wird ein Entscheidungsbaum unausblanciert, da Attribute im Modell mehr Gewichtung erhalten als sie eigentlich haben sollten. 