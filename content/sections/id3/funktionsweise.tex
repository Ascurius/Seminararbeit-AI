\section{Grundlagen}
\label{id3:funktionsweise}

Der ID3-Algorithmus macht sich zwei Konzepte der Informationstheorie zu nutze. Es handelt sich zum einen um die Entropie und zum anderen um den Informationsgewinn. Beide Konzepte werden im nachfolgenden erläutert. Im Anschhluss daran wird die eigentliche Funktionsweise des ID3 Algorithmus erläutert.

\subsection{Entropie}
\label{id3:entropie}
In der Informationstheorie wird mit der Entropie \textit{H} die Sicherheit bzw. Unsicherheit  einer Variablen \textit{X} angegeben. Dementsprechend ist $x_{i}$ eine mögliche Ausprägung der Variablen \textit{X} und \textit{P}($x_{i}$) die Wahrscheinlichkeit mit der die Variable \textit{X} die Ausprägung $x_{i}$ hat. \autocite{Entropy:online}

\begin{figure}[htbp]
    \vspace{0.5cm}
    \centering
    $ H(X) = - \sum\limits_{i=1}^{n} P(x_{i}) \log_{b} P(x_{i}) $
    \caption{Definition der Entropie nach Shennon\autocite{Entropy:online}}
\end{figure}

\textbf{\underline{Beispiel}:} Sei \textit{D} ein Datensatz in dem  das Attribut \textit{X} mit den möglichen Ausprägungen $x_{1}, x_{2}$ und $x_{3}$ vorkommt. Weiterhin gelte, dass $x_{1}$ neun mal, $x_{2}$ drei mal und $x_{3}$ fünf mal in \textit{D} vorhanden ist. Zur Bestimmung der Entropie von \textit{X} egribt sich die nachfolgende Berechnung. Für den ID3-Algorithmus wird üblicherweise der Logarithmus zur Basis \textit{b=2} verwendet. \autocite{ImplementationID3}

\begin{figure}[htbp]
    \centering
    \begin{align*}
         H(X)   &= - \sum\limits_{i=1}^{3} P(x_{i}) \log_{2} P(x_{i})\\
                &= - (P(x_{1}) \log_{2} P(x_{1}) + P(x_{2}) \log_{2} P(x_{2}) + P(x_{3}) \log_{2} P(x_{3}))\\
                &= -\frac{9}{17} \cdot \log_{2} \left(\frac{9}{17}\right) - %
                    \frac{3}{17} \cdot \log_{2} \left(\frac{3}{17}\right) - %
                    \frac{5}{17} \cdot \log_{2} \left(\frac{5}{17}\right)\\
                &\approx 0,485755 + 0,441618 + 0,519275\\
                &\approx \underline{\underline{1,446648}}
    \end{align*}
    \caption{Exmeplarische Berechung der Entropie}
\end{figure}
\pagebreak
\subsection{Informationsgewinn}
\label{id3:gain}
In der Informationstheorie beschreibt der Informationsgewinn \textit{IG} das Maß an Informationen das über eine Zufallsvariablen \textit{X} durch Beobachtung einer anderen Zufallsvariablen \textit{Y} gewonnen werden kann. \autocite{DataMining} Konkret ergibt sich der Informationsgewinn aus der Differenz der Entropie \textit{H}(\textit{X}) und der bedingten Entropie \textit{H}(\textit{X}$\vert$\textit{Y}). \autocite{Informat29:online}

\begin{figure}[H]
    \label{fig:gain}
    \vspace{0.5cm}
    \centering
        $ IG(X,Y) = H(X) - H(X\vert Y) = H(X) - \sum\limits_{y\in Y} P(y)H(X\vert Y=y)$
    \caption{Allgemeine Definition des Informationsgewinns \autocites{DataMining}{ConditionalEntropy:online}{BedingteEntropie:online}}
\end{figure}

\textbf{\underline{Beispiel}:} Sei \textit{S} ein Datensatz mit den in Tabelle \ref{table:gain} dargestellten Werten. Außerdem seien \textit{A}, \textit{B}, \textit{C} und \textit{T} Attribute von \textit{S} mit den möglichen Ausprägungen \textit{True} und \textit{False}. Sei weiterhin das Attribut \textit{T} das Zielattribut gegen das der Informationsgewinn der übrigen Attribute ermittelt werden soll.

\begin{center}
    \begin{table}[H]
        \centering
        \begin{tabularx}{0.8\linewidth}{ccccc}
            \toprule
            \textbf{ID} & \textbf{Attribut A} & \textbf{Attribut B} & \textbf{Attribut C} & \textbf{Attribut T}\\
            \toprule

            1 & True    & True  & True  & False \\
            2 & True    & False & True  & True \\
            3 & False   & False & True  & True \\
            4 & False   & True  & True  & False \\
            5 & False   & True  & False & True \\

            %1 & $a_{1}$ & $b_{2}$ & $c_{3}$ & True  \\
            %2 & $a_{1}$ & $b_{2}$ & $c_{3}$ & False \\
            %3 & $a_{2}$ & $b_{2}$ & $c_{1}$ & False \\
            %4 & $a_{1}$ & $b_{2}$ & $c_{1}$ & True  \\
            %5 & $a_{2}$ & $b_{1}$ & $c_{3}$ & False \\ 
            %6 & $a_{2}$ & $b_{3}$ & $c_{1}$ & False \\
            %7 & $a_{1}$ & $b_{2}$ & $c_{2}$ & True  \\  
            %8 & $a_{3}$ & $b_{3}$ & $c_{3}$ & False  \\
            \bottomrule
        \end{tabularx}
        \caption{Beispiel Datensatz \textit{S} zur Berechnung des Informationsgewinns \autocite{Informat29:online}}
        \label{table:gain}
    \end{table}
\end{center}

Im nachfolgenden wird der Informationsgewinn für das Attribut \textit{A} berechnet. Dabei wird zu erst die Entropie des Datensatz \textit{S} bestimmt, welche durch die Entropie des Zielattributes \textit{T} charakterisiert wird. Es gilt also $H(S) = H(T)$.

\begin{figure}[htbp]
    \centering
    \begin{align*}
        H(S) = H(T) &= - \sum\limits_{i=1}^{2} P(x_{i}) \log_{2} P(x_{i}) \\
                    &= - ( P(\text{True}) \log_{2} P(\text{True}) + P(\text{False}) \log_{2} P(\text{False}) ) \\
                    &= - \frac{3}{5} \cdot \log_{2} \left(\frac{3}{5}\right) - \frac{2}{5} \cdot \log_{2} \left(\frac{2}{5}\right)\\
                    &\approx 0,970951
    \end{align*}
\end{figure}

Im nächsten Schritt wird die bedingte Entropie für das Attribut \textit{A} berechnet. \autocites{BedingteEntropie:online}{ConditionalEntropy:online}

\begin{figure}[htbp]
    \centering
    \begin{align*}
        H(T\vert A) &= \sum\limits_{a\in A}P(A=a) \cdot H(T\vert A=a) \\
                    &= P(A=True) \cdot H(T\vert A=True) + P(A=False) \cdot H(T\vert A=False) \\
                    &= \frac{2}{5} \cdot \left( -\frac{1}{2} \cdot \log_2\left( \frac{1}{2} \right) - \frac{1}{2} \cdot \log_{2}\left( \frac{1}{2} \right) \right) + \frac{3}{5} \cdot \left( -\frac{1}{3} \cdot \log_{2}\left( \frac{1}{3} \right) - \frac{2}{3} \cdot \log_{2}\left( \frac{2}{3} \right) \right) \\
                    &= \frac{2}{5} \cdot 1 + \frac{3}{5} \cdot 0,918296\\
                    &\approx 0.950978
    \end{align*}
\end{figure}

Nachdem nun sowohl die Entropie als auch die bedingte Entropie berechnet sind kann final der Informationsgewinn bestimmt werden.

\begin{figure}[h]
    \vspace{0.5cm}
    \centering
    \begin{align*}
        IG(T,A) &= H(T) - H(T\vert A) \\
                &= 0,970951 - 0.950978 \\
                &= \underline{\underline{0,019973}}
    \end{align*}
\end{figure}

Analog können die Informationsgewinne für die Attribute \textit{B} und \textit{C} berechnet werden. Diese liegen bei $ IG(T, B) = \textit{0,419973} $ und bei $ IG(T, C) = \textit{0,170951} $.

\subsection{Funktionsweise}
Der ID3 Algorithmus startet mit einem Datensatz \textit{S} mit Objekten $O_{1},...,O_{n}$ welche verschiedenen Attribute $A_{1},...,A_{k}$ und eine Klassifizierung \textit{C} besitzen. \autocites{ImplementationID3}{QuinlanID3} Die Werteausprägungen der Attribute sind dabei normalerweise endlich und diskret. \autocite{ThailandID3}\\
Zu Beginn muss zunächst Knoten bestimmt werden. Dazu wird der Informationsgewinn \textit{IG}(\textit{S}) für jedes Attribut $A_{1},...,A_{k}$ berechnet. Das Attribut $A_{H}$ mit dem höchsten Informationsgewinnungswert wird dann als Knoten gewählt. Dabei gilt das $A_{H}$ die möglichen Ausprägungen $a_{1},...,a_{v}$ haben kann. Basierend auf $A_{H}$ wird \textit{S} in $S_{1},...,S_{v}$ Teilmengen zerlegt wobei die Teilmenge $S_{i}$ die Objekte aus \textit{S} beinhaltet deren Wert in $A_{H}$ die Ausprägungen $a_{i}$ hat. Anders ausgedrückt gilt $S_{i} = \{S\vert A_{H} = a_{i}\}$. Für jede Teilmenge $S_{i}$ wird dann rekursiv der vorher beschriebene Prozess für die verleibenden Attribute $\{A_{1},...,A_{k}\}\setminus A_{H}$ durchlaufen wodurch entsprechende Teilbäume erzeugt werden. \Autocite{QuinlanID3} Dabei wird die Rekursion beendet sofern:

\begin{enumerate}
    \item alle Objekte aus $S_{i}$ die gleiche Ausprägung $C_{i}$ der Klassifizierung \textit{C} besitzen. In diesem Fall wird der zu diesem Zeitpunkt ausgewählte Knoten zu einem Blatt mit der Ausprägung $C_{i}$. \autocites{QuinlanID3}{ID3algor15:online}
    \item bereits alle Attribute $A_{1},...,A_{k}$ für den Entscheidungsbaum verwendet worden sind, es also keine Attribute mehr gibt mittels derer ein Objekt aus $S_{i}$ klassifiziert werden könnte, aber nicht alle Objekte der selben Klassifizierung $C_{i}$ zugeordnet werden können. In diesem Fall wird der aktuelle Knoten zu einem Blatt, welchem der Modalwert von \textit{C} zugeordnet wird.\autocites{QuinlanID3}{ID3algor15:online}
    \item die Teilmenge $S_{i}$ keine Objekte mehr beinhaltet, sie also leer ist. Dies tritt ein wenn sich in der übergeordneten Menge keine Objekte befinden welche die Ausprägung $a_{i}$ eines Attributes $A_{M}$ besitzen. In diesem Fall wird ein Blatt erzeugt welchem der Modalwert von \textit{C} aus der übergeordneten Menge zugeordnet wird. \autocite{ID3algor15:online}
\end{enumerate}

\subsection{Eigenschaften von ID3}
\label{id3:eigenschaften}
Der ID3-Algorithmus führt eine Best-First-Search für lokale Optima durch. Außerdem ist ID3 ein gieriger Algorithmus, da er stets das Attribut auswählt welches lokal den besten Informationsgewinn aufweist.

\textbf{TODO: Vervollständigen}