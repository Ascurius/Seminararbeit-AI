\chapter{Pruning}
\label{pruning}
Beim Pruning handelt es sich um eine Methode zur Bearbeitung eines Entscheidungsbaums. Das Ziel ist dabei die Größe eines Baums zu reduzieren um so dem die Auswirkungen des \textit{Overfittings} zu minimieren. Die Prädiktive Qualität des Baums soll allerdings nicht unter Pruning leiden. \autocites{WikiPruning:online}{DataMining} Man unterscheidet dabei zwsichen dem \textit{Pre-Pruning} und dem \textit{Post-Pruning}. \autocite{DataMining}

\section{Pre-Pruning}
\label{pre-pruning}
Beim \textit{Pre-Pruning} erfolgt die Reduktion des Entscheidungsbaums noch bevor dieser fertiggestellt ist, bzw. während dieser erzeugt wird. Dabei soll verhindert werden dass unnötige Entscheidungsknoten und Teilbäume erzeugt werden, weshalb \textit{Pre-Pruning} einsetzt, wenn der Datensatz geteilt wird. Um Festzustellen ob der Datensatz geteilt werden sollte können verschiedene Kriterien zum Einsatz kommen. Zum Beispiel könnte das Kriterium des minimalen Informationsgewinns genutzt werden, bei welchem der Datensatz nur dann geteilt wird wenn der Wert des Informationsgewinn über einem bestimmten Mindestwert liegt. \autocite{DataMining} Allerdings unterliegen alle \textit{Pre-Pruning} Methoden dem \textit{Horizont Effekt}. \autocite{WikiPruning:online}\\
Dabei handelt es sich um ein Problem welches dazu führt dass eventuell ein Teilbaum erzeugt wird der nicht wünschenswert ist. Dies liegt daran, dass beim \textit{Pre-Pruning} nur der aktuelle Entscheidungsknoten betrachtet wird und wir nicht über diesen hinaus schauen können. \autocites{HorizonProblem}{WikiPruning:online} Der \textit{Horizont Effekt} lässt sich anhand des folgenden Beispiels veranschaulichen.\\
Nehmen wir an dass wir uns gerade innerhalb eines Entscheidungsbaums an einem Entscheidungsknoten befinden und bestimmen wollen wie der Trainingsdatensatz aufgeteilt werden soll. Dabei nutzen wir das Kriterium des minimalen Informationsgewinns. Also berechnen wir die Informationsgewinne der verschiedenen Attribute \textit{A}, \textit{B} und \textit{C}. Wir stellen dabei fest, dass der Informationsgewinn der Attribute \textit{A} und \textit{C} unterhalb des von uns festgelegten Mindestwertes liegt. Aus diesem Grund wählen wir das Attribut \textit{B} für den Entscheidungsknoten aus. Was zu diesem Zeitpunkt allerdings nicht klar ist, ist dass nach dem Entscheidungsknoten mit dem Attribut \textit{A}, zwei weitere Entscheidungsknoten mit den Attributen $A_{1}$ und $A_{2}$ gefolgt wären, welche den Datensatz besser klassifiziert hätten als das Attribut \textit{B}.\\

\section{Post-Pruning}
\label{post-pruning}
Die Reduktion der Baumgröße erfolgt beim \textit{Post-Pruning} nachdem der Entscheidungsbaum fertiggestellt wurde. Dabei ist die grundlegende Idee, dass Entscheidungsknoten bzw. Teilbäume zu Blättern umgewandelt werden. Erreicht werden kann dies durch verschiedene Methoden, welche sich in die zwei Kategorien \textit{Bottom-Up Post-Pruning} und \textit{Top-Down Post-Pruning} einteilen lassen können. \autocite{WikiPruning:online}\\
Wie der Name es bereits vermuten lässt, handelt es sich bei der \textit{Bottom-Up} um eine Pruning Vorgehensweise welche bei dem tiefsten Blatt des Entscheidungsbaums startet. Sofern sich zwei Blätter auf der gleichen Ebene befinden, wird das Blatt gewählt welches sich am weitesten Links befindet. \autocite{SebastianManteyPruning:online} Von diesem Blatt dem Baum aufwärts folgend, wird die Relevanz jedes Entscheidungsknoten für die vorliegende Klassifikation bestimmt. Falls ein Entscheidungsknoten als irrelevant bewertet wird, so wird dieser durch ein Blatt ersetzt. Einer der bekanntesten und einfachsten \textit{Bottom-Up Post-Pruning} Algorithmen ist der \textit{Reduced-Error-Pruning} Algorithmus, welcher einem Blatt den Modalwert der Klassifikation zuweist. \autocite{SebastianManteyPruning:online}\\
Dem gegenüber steht die Vorgehensweise des \textit{Top-Down Post-Prunings} Prunings. Diese Form des \textit{Post-Prunings} startet an dem Wurzelknoten und arbeitet sich abwärts zu den Blättern vor. Auch hier wird analog zum \textit{Bottom-Up} überprüft ob ein Entscheidungsknoten relevant für die Klassifikation ist. Problematisch ist dabei, dass durch das entfernen einens Entscheidungsknoten unter Umständen ein kompletter Teilbaum entfernt wird, obwohl unklar ist obwohl unklar ist ob dieser von Relevanz für die Klassifikation ist. \autocites{WikiPruning:online}{SebastianManteyPruning:online}